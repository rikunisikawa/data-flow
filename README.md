# data-flow
ãƒ‡ãƒ¼ã‚¿åŸºç›¤å…¨èˆ¬ã‚’æ§‹ç¯‰

# âœ… ãƒ‡ãƒ¼ã‚¿åŸºç›¤æ§‹ç¯‰ãƒ•ãƒ­ãƒ¼è¨­è¨ˆä»•æ§˜æ›¸

## ğŸ¯ ç›®çš„

KaggleHubçµŒç”±ã§å–å¾—ã—ãŸmHealthãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ETLå‡¦ç†ã—ã€Athenaã§åˆ†æå¯èƒ½ãªå½¢å¼ï¼ˆParquetï¼‹é©åˆ‡ãªã‚¹ã‚­ãƒ¼ãƒï¼‰ã§S3ã«æ ¼ç´ã™ã‚‹è‡ªå‹•åŒ–ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

---

## ğŸ—ºï¸ å…¨ä½“æ§‹æˆå›³ï¼ˆå‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼‰

**EventBridge Scheduler**  
â†’ å®šæ™‚å®Ÿè¡Œï¼ˆä¾‹ï¼šæ¯æ—¥8:00ï¼‰  
â†’ **Step Functions**ã‚’ãƒˆãƒªã‚¬ãƒ¼

**Step Functions ã‚¹ãƒ†ãƒƒãƒ—**
1. Lambdaâ‘  â†’ ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»S3ä¿å­˜  
2. Lambdaâ‘¡ ã¾ãŸã¯ Glueâ‘  â†’ CSV â†’ Parquetå¤‰æ›  
3. Glueâ‘¡ â†’ ãƒ‡ãƒ¼ã‚¿æ•´å½¢ â†’ S3ï¼ˆåˆ†æç”¨ãƒ‘ã‚¹ï¼‰ã«ä¿å­˜

**Athena**  
â†’ Glue Data Catalogã«ã‚ˆã‚ŠS3ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã«ã‚¯ã‚¨ãƒªå¯èƒ½

---

## ğŸ“¦ ä½¿ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã¨å½¹å‰²

| ã‚µãƒ¼ãƒ“ã‚¹       | å½¹å‰²                                       |
|----------------|--------------------------------------------|
| EventBridge    | å®šæ™‚ãƒˆãƒªã‚¬ãƒ¼                               |
| Step Functions | å‡¦ç†ãƒ•ãƒ­ãƒ¼åˆ¶å¾¡                             |
| Lambdaâ‘         | KaggleHubã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã—ã€S3ä¿å­˜            |
| Lambdaâ‘¡ / Glueâ‘ | CSV â†’ Parquetå¤‰æ›                          |
| Glueâ‘¡          | ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»æ•´å½¢ãƒ»ã‚«ã‚¿ãƒ­ã‚°æ›´æ–°             |
| Athena         | ã‚¯ã‚¨ãƒªãƒ»åˆ†æå®Ÿè¡Œ                           |

---

## ğŸ§± S3 æ§‹æˆä¾‹

```
s3://your-bucket-name/
â”œâ”€â”€ raw/             â† Lambdaâ‘ ä¿å­˜ (CSV)
â”œâ”€â”€ stage/           â† Lambdaâ‘¡ã¾ãŸã¯Glueâ‘  (Parquet)
â””â”€â”€ processed/       â† Glueâ‘¡å‡ºåŠ› (æ•´å½¢æ¸ˆParquet)
```

---

## ğŸ’» ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º & ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †

### âœ… ã‚¹ãƒ†ãƒƒãƒ—0ï¼šå‰æãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

- Python 3.11+
- Docker
- AWS CLI & `aws configure`
- AWS SAM CLI (`brew install aws/tap/aws-sam-cli`)
- `kagglehub` ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼š`pip install kagglehub`

---

### âœ… ã‚¹ãƒ†ãƒƒãƒ—1ï¼šSAM ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ

```bash
sam init
# â†’ runtime: python3.11
# â†’ template: Hello World Example
```

---

### âœ… ã‚¹ãƒ†ãƒƒãƒ—2ï¼šLambdaâ‘  ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—ï¼†S3ä¿å­˜ï¼‰

```python
# lambda/download_and_upload.py
import json
import boto3
import kagglehub
from pathlib import Path
import os

s3 = boto3.client('s3')
BUCKET = os.environ['BUCKET_NAME']

def lambda_handler(event, context):
    path = kagglehub.dataset_download("nirmalsankalana/mhealth-dataset-data-set")
    local_path = Path(path)
    for file in local_path.glob("**/*.csv"):
        s3.upload_file(str(file), BUCKET, f"raw/{file.name}")
    return {"status": "uploaded", "files": len(list(local_path.glob('*.csv')))}
```

---

### âœ… ã‚¹ãƒ†ãƒƒãƒ—3ï¼šLambdaâ‘¡ï¼ˆCSV â†’ Parquetï¼‰ or Glueâ‘  ã‚’ä½œæˆ

**Lambdaç‰ˆï¼š`pandas + pyarrow` ã‚’ä½¿ã†**

```bash
pip install pandas pyarrow -t lambda/convert/
```

```python
# lambda/convert_csv_to_parquet.py
import boto3
import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa
import os
from io import BytesIO

s3 = boto3.client("s3")
BUCKET = os.environ['BUCKET_NAME']

def lambda_handler(event, context):
    response = s3.list_objects_v2(Bucket=BUCKET, Prefix='raw/')
    for obj in response.get('Contents', []):
        key = obj['Key']
        if key.endswith('.csv'):
            csv_obj = s3.get_object(Bucket=BUCKET, Key=key)
            df = pd.read_csv(csv_obj['Body'])
            table = pa.Table.from_pandas(df)
            parquet_buffer = BytesIO()
            pq.write_table(table, parquet_buffer)
            s3.put_object(Bucket=BUCKET, Key=key.replace("raw/", "stage/").replace(".csv", ".parquet"),
                          Body=parquet_buffer.getvalue())
```

---

### âœ… ã‚¹ãƒ†ãƒƒãƒ—4ï¼šGlueâ‘¡ ä½œæˆï¼ˆæ•´å½¢ãƒ»ã‚«ã‚¿ãƒ­ã‚°åŒ–ï¼‰

- ãƒ‡ãƒ¼ã‚¿ã®å‹å¤‰æ›ï¼ˆä¾‹ï¼štimestampå¤‰æ›ã€ã‚«ãƒ©ãƒ åæ­£è¦åŒ–ï¼‰
- `processed/` ã¸Parquetå½¢å¼ã§ä¿å­˜
- Glue Data Catalog ã«ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ

---

### âœ… ã‚¹ãƒ†ãƒƒãƒ—5ï¼šStep Functions å®šç¾©ï¼ˆtemplate.yamlï¼‰

```yaml
Resources:
  Workflow:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      DefinitionString:
        Fn::Sub: |
          {
            "StartAt": "DownloadCSV",
            "States": {
              "DownloadCSV": {
                "Type": "Task",
                "Resource": "${DownloadFunction.Arn}",
                "Next": "ConvertParquet"
              },
              "ConvertParquet": {
                "Type": "Task",
                "Resource": "${ConvertFunction.Arn}",
                "Next": "GlueTransform"
              },
              "GlueTransform": {
                "Type": "Task",
                "Resource": "arn:aws:states:::glue:startJobRun.sync",
                "Parameters": {
                  "JobName": "YourGlueJobName"
                },
                "End": true
              }
            }
          }

  DownloadFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: lambda/
      Handler: download_and_upload.lambda_handler
      Runtime: python3.11
      Environment:
        Variables:
          BUCKET_NAME: your-bucket-name
```

---

### âœ… ã‚¹ãƒ†ãƒƒãƒ—6ï¼šãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆ

```bash
sam build
sam local invoke DownloadFunction
```

---

### âœ… ã‚¹ãƒ†ãƒƒãƒ—7ï¼šãƒ‡ãƒ—ãƒ­ã‚¤

```bash
sam deploy --guided
```

---

### âœ… ã‚¹ãƒ†ãƒƒãƒ—8ï¼šAthenaè¨­å®š

Glue Data Catalogã«Parquetãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã€‚ä»¥ä¸‹ã®DDLã‚’å®Ÿè¡Œï¼š

```sql
CREATE EXTERNAL TABLE mhealth (
  id string,
  timestamp timestamp,
  accel_x double,
  accel_y double,
  accel_z double
)
STORED AS PARQUET
LOCATION 's3://your-bucket-name/processed/';
```

---

## ğŸ“Œ è£œè¶³

- Glueã®Sparkå‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ã§ã€å¤§é‡ãƒ‡ãƒ¼ã‚¿ã¸ã®å¯¾å¿œãŒå®¹æ˜“ã«ãªã‚Šã¾ã™ã€‚
- Lambdaã§Parquetå¤‰æ›ã™ã‚‹å ´åˆã¯ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«æ³¨æ„ï¼ˆ512MBã€œ1GBæ¨å¥¨ï¼‰ã€‚
